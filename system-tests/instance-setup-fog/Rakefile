require "rubygems"
require "bundler/setup"
require 'excon'
require 'fog'
require 'resolv'
require 'pp'


#We want the output immediatly, nothing performance critical for this task anyway
$stdout.sync = true

if ENV['EC2_ACCESS_KEY'] and ENV['EC2_SECRET_KEY']
  credentials_ec2 = {:provider => 'AWS', :aws_access_key_id => ENV['EC2_ACCES_KEY'], :aws_secret_access_key => ENV['EC2_SECRET_KEY'] }
else
  require 'net/netrc'
  rc = Net::Netrc.locate('ec2.client')
  if rc
    puts "Succesfully loaded netrc credentials_ec2."
  else
    raise ".netrc missing or no entry found."
  end
  credentials_ec2 = {:provider => 'AWS', :aws_access_key_id => rc.login, :aws_secret_access_key => rc.password}
  credentials_dynect = Net::Netrc.locate('nettica.client')
  unless credentials_dynect
    puts "************************Warning: No Dynect credentials found, skipping DNS steps************************************"
  end
end

#This is a way to tell fog what kind of name we want to give the key
#If we don't pass :key_name to a bootstrap() call,
#this will be used to create a key with Amazon titled fog_$ourname later
Fog.credential = 'gardens_qa_key'
#This is used during the bootstrap process.

#We could pass this to the bootstrap() call, but there is currently a bug
#In Fog that keeps us from doing that for spot instances: https://github.com/fog/fog/issues/591
Fog.credentials = Fog.credentials.merge({
  :private_key_path => "./keys/gardens_qa_id_rsa",
  :public_key_path => "./keys/gardens_qa_id_rsa.pub"
})

connection = Fog::Compute.new(credentials_ec2)
#Only necessary if we don't pass the keys to the bootstrap() method
#--> https://github.com/fog/fog/issues/591
connection.import_key_pair('gardens_qa_key', IO.read('./keys/gardens_qa_id_rsa.pub')) if connection.key_pairs.get('gardens_qa_key').nil?

desc 'Launch a new EC2 machine and provision Gardens on it'
task :setup_gardens_instance, :subdomain, :instance_size, :spot_or_ondemand, :devel_module do |t, args|
  subdomain = args[:subdomain].to_s.downcase
  full_hostname = "#{subdomain}.gardensqa.acquia-sites.com"
  instance_size = args[:instance_size].to_s.downcase
  spot_or_demand = args[:spot_or_ondemand].to_s.downcase
  devel_module = args[:devel_module].to_s.downcase
  if subdomain.empty?
    puts "You didn't pass subdomain environment"
    exit(1)
  end
  if instance_size.empty?
    puts "You didn't pass the instance size you want."
    exit(1)
  end

  ok_instance_sizes = ['m1.small', 'c1.medium', 'm1.large']
  if not ok_instance_sizes.include?(instance_size)
    puts "Sorry, your chosen instance size #{instance_size.inspect} isn't on our list of ok instance sizes: #{ok_instance_sizes.inspect}"
    exit(2)
  end

  puts "Booting a VM for Gardens installation"
  #launch lucid, instance store, us1-east
  #lucid in US east: ami-6936fb00
  #hardy in US east: 32 bit: ami-91579af8 | 64 bit: ami-a5569bcc

  if ["m1.large"].include?(instance_size)
    #64 bit
    cpu_architecture = 64
    chosen_image_id = 'ami-a538e1cc'
    current_distro = 'hardy'
  else
    #32 bit
    cpu_architecture = 32
    chosen_image_id = 'ami-a539e0cc'
    current_distro = 'hardy'
  end

  tags_for_the_server = {:type => 'gardens_qa_server', :instance_size => instance_size, :instance_pricing => spot_or_demand, :acquia_dns => full_hostname, :creation_time => Time.now.to_s}
  #jenkins eports this variable. Manually launched servers won't have this.
  if ENV['BUILD_TAG']
    tags_for_the_server[:build_tag] = ENV['BUILD_TAG']
  end

  server_attributes = {
    :flavor_id => instance_size,
    :tags => tags_for_the_server,
    :image_id => chosen_image_id,
    :key_name => 'gardens_qa_key',
    :groups => ['fields_dev'] #this is the default sec group that allows,among other things, http access to the server
  }


  #boot and install SSH keys on there
  puts "Bootstrapping server."
  begin
    case spot_or_demand
    when 'spot'
      #For now this is 1, we should probably try to figure out how to bet double the current rate?
      spot_bidding_attributes = {:price=>"1"}
      spot_server_attributes = server_attributes.merge(spot_bidding_attributes)
      puts "Launching a spot instance with the following attributes: #{spot_server_attributes.inspect}."
      server = connection.spot_requests.bootstrap(spot_server_attributes)
    when 'ondemand'
      puts "Launching an on-demand instance"
      server = connection.servers.bootstrap(server_attributes)
    else
      raise "Unknown instance payment type: #{spot_or_demand}"
    end

    puts "Finished baking the server:\n#{server.inspect}"
    if credentials_dynect and ENV['NO_DNS_SETUP'].nil?
      puts "Setting DNS name to #{full_hostname}."

      dynect_user = credentials_dynect.login
      dynect_pass = credentials_dynect.password

      tried_dns_cleanup = false

      begin
        dynect = Fog::DNS.new(:provider => "dynect", :dynect_customer => "acquia", :dynect_username => dynect_user, :dynect_password => dynect_pass)

        zone = dynect.zones.get("acquia-sites.com")
        # create A record
        record = zone.records.create(
          :rdata => {'cname' => server.dns_name},
          :name => full_hostname,
          :type => 'CNAME',
          :ttl => 60 #short TTL so we don't have to wait ages if we switch
        )
        puts "Created DNS record: #{full_hostname.inspect} => #{server.dns_name.inspect}"
      rescue StandardError => e
        if tried_dns_cleanup
          raise e
        else
          retried_dns_cleanup = false
          puts "Received an error while trying to set the DNS record: #{e.message}"
          begin
            puts "Deleting DNS record first and then giving it a second try"

            record = dynect.get_record('CNAME', 'acquia-sites.com', full_hostname, {})
            matching_record_id = record.body['data'].first.to_s.split('/').last.to_s
            if matching_record_id.empty?
              puts "No record matching #{full_hostname.inspect} found."
              exit(2)
            else
              #remove the record from the zone
              dynect.delete_record("CNAME", "acquia-sites.com", full_hostname, matching_record_id)
              #publish the zone so the record deletion takes effect
              zone.publish
              puts "Deleted record #{full_hostname.inspect}."
            end
          rescue StandardError => e
            if retried_dns_cleanup
              raise e
            else
              puts "Retrying DNS cleanup in 10s."
              retried_dns_cleanup = true
              sleep 10
              retry
            end
          end
          tried_dns_cleanup = true
          retry
        end
      end #dns setup
    end
    puts "Checking ssh."
    ssh_results = server.ssh("uname -a")
    puts "Uname returns: #{ssh_results.first.stdout}"

    if current_distro == 'hardy'
      puts "Adding a ruby 1.8.7 repository for hardy"
      server.ssh("sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B02E741")
      server.ssh('sudo touch /etc/apt/sources.list.d/ruby.list; echo "deb http://ppa.launchpad.net/ubuntu-ruby/ppa/ubuntu hardy main" | sudo tee /etc/apt/sources.list.d/ruby.list > /dev/null')
      puts "Adding the git ppa repository for hardy"
      server.ssh("sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E1DF1F24")
      server.ssh('sudo touch /etc/apt/sources.list.d/git.list; echo "deb http://ppa.launchpad.net/git-core/ppa/ubuntu hardy main" | sudo tee /etc/apt/sources.list.d/git.list > /dev/null')
    end


    puts "Updating repos."
    server.ssh("sudo apt-get update")
    puts "Installing dependencies needed for chef + the gardens ruby installer (ruby, gcc, ...)"
    #ruby1.8-dev for nokogiri later, JRE for akephalos later
    #now handled by chef: git-core
    server.ssh("sudo apt-get install build-essential htop libxml2-dev libxslt1-dev libxslt1.1 ruby1.8 ruby1.8-dev openjdk-6-jre-headless rsync vim curl libopenssl-ruby --force-yes --yes")
    puts "Updating rubygems"

    server.scp("dependencies/rubygems-1.8.11.tgz", "/home/ubuntu/")
    server.ssh("tar xfvz rubygems-1.8.11.tgz; cd rubygems-1.8.11; sudo ruby setup.rb")
    server.ssh('echo "gem: --bindir /usr/local/bin" >> ~/.gemrc')

    puts "Installing chef."
    gem_results = server.ssh("sudo gem1.8 install chef --no-rdoc --no-ri")
    puts "Done installing chef."
    puts "Copying chef recipes"
    sh "tar zcf chef-solo.tar.gz ./chef_data"
    server.scp("chef-solo.tar.gz", "/home/ubuntu/")
    sh "rm chef-solo.tar.gz"
    server.ssh("cd /home/ubuntu; tar xfz chef-solo.tar.gz")
    puts "Running chef to install LAMP stack."
    chef_results = server.ssh("cd /home/ubuntu/chef_data; sudo chef-solo -c solo.rb -j node.json")
    if chef_results.first.stdout.include?("Chef Run complete")
      puts "LAMP stack set up successfully."
    else
      puts "Did not receive the 'Chef Run complete' message:\n#{chef_results.first.stdout}"
      exit(1)
    end

    url = "http://#{server.dns_name}/"
    puts "Checking for Apache install success on #{url}."
    response = Excon.get(url)
    if response.status == 200
      puts 'Got an OK from the webserver'
    else
      puts "Got the following answer from the freshly installed webserver at #{url}: #{response.status}."
      exit(1)
    end

    puts "Restarting mysql (race condition bug in hardy)."
    #2 of them are running, one of them just eats CPU
    server.ssh("sudo killall -9 mysqld_safe")
    server.ssh("sudo /etc/init.d/mysql restart")

    puts "tar'ing up gardens_docroot"
    sh "tar -C ../../docroot -czf gardens_docroot.tar.gz ."

    puts "Setting up docroot"
    server.ssh("sudo rm -rf /var/www/*")
    server.scp("gardens_docroot.tar.gz", "/home/ubuntu/")
    sh "rm gardens_docroot.tar.gz"
    server.ssh("sudo tar xzf ~/gardens_docroot.tar.gz -C /var/www/")
    server.ssh("sudo chown -R www-data:www-data /var/www")

    puts "Setting up mySQL database"
    server.ssh("mysql --user=root -e 'DROP DATABASE IF EXISTS gardens_trunk; CREATE DATABASE gardens_trunk;'")

    puts "Setting up gardens installation"
    server.scp("../../install_gardens.php", "/home/ubuntu/")

    puts "Creating site profile"
    server.ssh("sudo -u www-data mkdir -p /var/www/sites/#{full_hostname}/themes/mythemes")
    server.ssh("sudo -u www-data cp /var/www/sites/dev.example/settings.php /var/www/sites/#{full_hostname}/")

    puts "Creating link to work around installer problems"
    server.ssh("sudo -u www-data ln -s /var/www/docroot /var/www/")

    setup_command = <<-EOC
    cd /var/www && sudo -u www-data php -d memory_limit=128M $HOME/install_gardens.php \
        database='gardens' \
        username='root' \
        password='' \
        profile='gardens' \
        url='http://#{full_hostname}' \
        site_template='campaign' \
        user2_name='#{ ENV['owner_name'] || "qatestuser" }' \
        user2_pass='#{ ENV['owner_pass'] || "ghetto#exits" }' \
        user2_mail='#{ ENV['owner_mail'] || "qa_admin@example.com" }'
    EOC

    puts "Running gardens installer: '#{setup_command}'"
    setup_output = server.ssh(setup_command)
    puts "Output of gardens installer:\n #{setup_output.first.stdout}"

    puts "Saving a mysql dump of the clean installation state"
    server.ssh("cd /var/www/; mysqldump -u root gardens_dev | sudo -u www-data tee gardens_original.sql > /dev/null")

    puts "Copying our testing shortcuts"
    server.scp("testing_shortcuts/qa_reset.php", "/home/ubuntu/")
    server.ssh("sudo -u www-data cp /home/ubuntu/qa_reset.php /var/www/")

    url = "http://#{full_hostname}/"
    puts "Checking for Drupal install success on #{url}."
    response = Excon.get(url)
    if response.body.match(/<body class="(.*)front(.*)not-logged-in(.*)" >/)
      puts "Detected a drupal installation on the webserver. (w00t w00t!). You may log in with username: admin and passord: admin\n"
    else
      puts "Didn't detect a drupal install on the webserver at #{url}.\nHTTP Code: #{response.status}.\nHTTP Body: #{response.body}"
      exit
    end

    puts "Chowning /usr/share/php/drush/lib"
    puts server.ssh("sudo chown -R www-data /usr/share/php/drush/lib").first.stderr

    puts "Verifying that the devel module isn't already enabled"
    module_query = "\"SELECT filename FROM system WHERE type = 'module' AND name = 'devel' AND status = 1;\""
    module_result = server.ssh("sudo -u www-data drush --root=/var/www/ --uri='http://#{full_hostname}' sql-query #{module_query}").first.stdout
    raise "Devel module was already enabled ... bailing out: #{module_result}" if module_result.length > 0

    puts "Enabling fast user switching"
    puts server.ssh("sudo -u www-data drush pm-download devel -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout
    puts server.ssh("sudo -u www-data drush pm-enable devel -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout
    # drush_role needs to write modules to /usr/share/drush/
    puts server.ssh("sudo drush pm-download drush_role-7.x-1.x -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout
    # We need to clear the cache before setting permissions
    puts server.ssh("sudo -u www-data drush cache-clear all -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout
    puts server.ssh("sudo -u www-data drush role-add-perm 1 'switch users' -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout
    puts server.ssh("sudo -u www-data drush role-add-perm 2 'switch users' -r /var/www/ --uri='http://#{full_hostname}' --yes").first.stdout

    puts "Verifying that the devel module was enabled successfully"
    module_result = server.ssh("sudo -u www-data drush --root=/var/www/ --uri='http://#{full_hostname}' sql-query #{module_query}").first.stdout
    raise "Devel module couldn't be enabled successfully ... bailing out: #{module_result}" if module_result.length == 0

    puts "Setting permissions"
    server.ssh("sudo chmod u+w /var/www/sites/*")

    puts "Cleaning up installation directory"
    cleanup_url = "http://#{full_hostname}/qa_reset.php?operation=cleanup_installation"
    cleanup_response = Excon.get(cleanup_url)
    puts cleanup_response.body

    puts "Creating an installation snapshot"
    snapshot_url = "http://#{full_hostname}/qa_reset.php?operation=create_snapshot"
    snapshot_response = Excon.get(snapshot_url)
    puts snapshot_response.body

  rescue StandardError => e
    puts "We failed, trying to clean up :("
    if server.nil?
      puts "Server doesn't seem to be available, can't kill it :("
    else
      puts "Server has instance ID: #{server.id}, trying to kill it."
      begin
        Rake::Task["kill_server_by_instance_id"].execute({:instance_id => server.id})
      rescue Exception => e
        puts "Killing server failed: #{e.message}"
      end
    end
    puts 'Reraising exception'
    raise e
  end

end

desc 'Check EC2 for running instances with gardens on it.'
task :list_all_gardens_servers do
  connection = Fog::Compute.new(credentials_ec2)
  server_resource_ids = []
  connection.tags.all(:key => "type", :value => "gardens_qa_server").each{|item| server_resource_ids << item.resource_id}
  if server_resource_ids.empty?
    puts "There are currently no servers with the 'gardens_qa_server' tag running."
  else
    connection.servers.all('instance-id' => server_resource_ids).table([:id, :flavor_id, :tags, :availability_zone, :image_id, :dns_name, :state ])
  end
end

desc 'Check EC2 for all running instances.'
task :list_all_servers do
  connection = Fog::Compute.new(credentials_ec2)
  connection.servers.all.table([:id, :flavor_id, :tags, :availability_zone, :image_id, :dns_name, :state ])
end

desc 'Check EC2 for all instances that have been running for over 24 hours.'
task :list_old_servers do
  connection = Fog::Compute.new(credentials_ec2)
  #connection.servers.all.table([:id, :flavor_id, :tags, :availability_zone, :image_id, :dns_name, :state ])
  old_servers = connection.servers.all.select {|server| server.state == 'running' && (((Time.now.to_i - server.created_at.to_i) / 60.0 / 60.0).to_i > 24) }
  unless old_servers.empty?
    puts "Theses #{old_servers.size} servers have been running for over 24 hours:"
    old_servers.each do |server|
      puts "Instance ID: #{server.id}"
      puts "Created at #{server.created_at}"
      age_in_seconds = Time.now.to_i - server.created_at.to_i
      age_in_hours = (age_in_seconds / 60.0 / 60.0).to_i
      puts "--> Age: #{'%0.2f' % (age_in_hours / 24.0).to_f} days"
      puts "Details:"
      pp server
      puts "**************************************"
    end
  end
  puts "#{old_servers.size} machines"
end

desc "Shut down a server by EC2 DNS name ('ec2-107-20-1-48.compute-1.amazonaws.com')."
task :kill_server_by_ec2_dns, :ec2_dns do |t, args|
  identifier = args[:ec2_dns]
  unless identifier
    puts "Error: You didn't pass the EC2 DNS name to the rake task."
    exit(1)
  end
  connection = Fog::Compute.new(credentials_ec2)
  server_resource_ids = []
  connection.tags.all(:key => "type", :value => "gardens_qa_server").each{|item| server_resource_ids << item.resource_id}
  if server_resource_ids.empty?
    puts "No active gardens_qa_servers found."
    exit(1)
  else
    puts "Found #{server_resource_ids.size} gardens servers."
    puts "Trying to destroy instance with DNS name: #{identifier.inspect}"
    shut_down_a_server = false
    dns_names = []
    server_resource_ids.each do |resource_id|
      server = connection.servers.get(resource_id)
      if server.dns_name == identifier
        puts "Found it, shutting down: #{identifier} (instance ID: #{server.id})"
        server.destroy
        shut_down_a_server = true
        break
      else
        #keep track of the currently running DNS names in case we don't find anything
        dns_names << server.dns_name unless server.dns_name.nil?
      end
    end
    if shut_down_a_server
      puts "Shut down this server: #{identifier.inspect}"
    else
      raise "Couldn't find a server with the dns name #{identifier.inspect} to shut down. Currently running servers: (#{dns_names.inspect})"
    end
  end
end

desc "Shut down a server by acquia DNS name ('testing.gardensqa.acquia-sites.com')."
task :kill_server_by_acquia_dns, :acquia_dns do |t, args|
  identifier = args[:acquia_dns]

  unless identifier
    puts "Error: You didn't pass the acquia DNS name to the rake task."
    exit(1)
  end

  dns_name = "#{identifier}.gardensqa.acquia-sites.com"

  #Kill the EC2 machine
  puts "Killing EC2 machine with DNS: #{dns_name}."
  servers = connection.tags.all(:key => "type", :value => "gardens_qa_server").map { |item| connection.servers.get(item.resource_id) }
  servers_to_kill = servers.select do |server|
    server.tags.has_key?("acquia_dns") and server.tags["acquia_dns"] == dns_name
  end

  servers_killed = false

  if servers_to_kill.empty? and dns = Resolv::DNS.open { |dns| dns.getresources(dns_name, Resolv::DNS::Resource::IN::CNAME) }.first
    ec2_dns =  dns.first.name.to_s
    puts "Killing EC2 machine with DNS: #{ec2_dns.inspect}."
    Rake::Task["kill_server_by_ec2_dns"].execute({:ec2_dns => ec2_dns})
    servers_killed = true
  else
    servers_to_kill.each do |server|
      puts "Found it, shutting down: #{dns_name} (instance ID: #{server.id})"
      server.destroy
      servers_killed = true
    end
  end

  raise "Couldn't find a server with the dns name #{dns_name} to shut down." unless servers_killed

  dns_result = Resolv::DNS.open { |dns| dns.getresources(dns_name, Resolv::DNS::Resource::IN::CNAME) }.first
  if dns_result
    ec2_dns = dns_result.name.to_s
    #Kill the DNS CNAME entry
    if credentials_dynect
      puts "Deleting DNS name #{dns_name} ."

      dynect_user = credentials_dynect.login
      dynect_pass = credentials_dynect.password
      dynect = Fog::DNS.new(:provider => "dynect", :dynect_customer => "acquia", :dynect_username => dynect_user, :dynect_password => dynect_pass)

      zone = dynect.zones.get("acquia-sites.com")

      #Marc: This is how we should usually do it, but at the moment that throws exceptions for me :(
      #record = zone.records.all.detect {|record| record.fqdn == identifier}.first
      #record.destroy

      record = dynect.get_record('CNAME', 'acquia-sites.com', dns_name, {})
      matching_record_id = record.body['data'].first.to_s.split('/').last.to_s
      if matching_record_id.empty?
        puts "No record matching '#{dns_name}' found."
        exit(2)
      else
        #remove the record from the zone
        dynect.delete_record("CNAME", "acquia-sites.com", dns_name, matching_record_id)
        #publish the zone so the record deletion takes effect
        zone.publish
        puts "Deleted record '#{dns_name}'."
      end
    else
      puts "No Dynect credentials found."
      exit(3)
    end
  else
    puts "Couldn't find a server with the cname: #{dns_name}"
    puts `dig #{dns_name}`
    exit(4)
  end
end


desc 'Shut down a server by instance ID (i-123abcdef).'
task :kill_server_by_instance_id, :instance_id do |t, args|
  identifier = args[:instance_id]
  unless identifier
    puts "Error: You didn't pass an instance ID to the rake task."
    exit(1)
  end
  puts "trying to locate and kill server"
  connection = Fog::Compute.new(credentials_ec2)
  server = connection.servers.get(identifier)
  server.destroy
  puts "Killed #{identifier.inspect}"
end
